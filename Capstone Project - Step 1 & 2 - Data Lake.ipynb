{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Lake - U.S. Immigration Data\n",
    "\n",
    "#### Data Engineering Capstone Project - Jonathan Cen\n",
    "\n",
    "#### Project Summary\n",
    "    \n",
    "- A research center decides to study what U.S. cities are among the most popular destinations for immigrants such that they can propose an infrastucutre upgrade plan to the local governemnt. An example of infrastructure upgrade could be constructing more large airports in states that are popular destinations of immigrants. As a data engineer at the research center, Jonathan is tasked to build a <code>data pipeline</code> to ingest raw data and construct both a Data Lake and a Data Warehouse to support analytical purposes for the research center's data sciecne team.\n",
    "\n",
    "- The datasets available to the research center are:\n",
    "    - I94 Immigration Data - this data comes from the US Natinoal Tourism and Trade Office.\n",
    "    - U.S. City Demographic Data - this data comes from OpenDataSoft\n",
    "    - Airport Code Table, provides details such as types, localtion, state code of an airport. Only airports within the U.S. are of interest at this stage.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import os\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col, udf, create_map, lit\n",
    "from valid_parameters import valid_i94addr, valid_i94port, valid_i94res_i94cit, valid_i94mode, i94port_map, i94cit_i94res_map, i94mode_map\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "datalake_url = \"s3a://udacity-capstone-jc-test/data-lake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "config = configparser.ConfigParser()\n",
    "config.read('capstone.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Instantiate a spark session for data cleansing and ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    ".config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.2\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()\n",
    "\n",
    "# bind AWS credentials to the spark session\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "    \n",
    "### Project Detail:\n",
    "- Raw data are stored in locally.\n",
    "- Data shall be cleaned, partitioned, and uploaded to an Amazon S3 data lake.\n",
    "- Create an Amazon Redshift data warehouse to store U.S. immigration data from 2016 and beyond.\n",
    "    - A <code>Star Schema</code> shall be used to create such data warehouse.\n",
    "    - Raw data lands on staging tables which will then be used for SQL to SQL ETL.\n",
    "- BI tools such as Amazon Quicksight could be used for final data visualization and reporting purposes.\n",
    "- Data science activities such as machine learning and deep learning can then be conducted with data stored in the data warehouse.\n",
    "\n",
    "\n",
    "### Data Use:\n",
    "- US Immigration Data in 2016 (structured data, <code>sas7bdat</code> format)\n",
    "- Airport code table (structured data, <code>csv</code> format)\n",
    "- U.S. city demographic data (semi-structured data, <code>csv</code> format)\n",
    "\n",
    "### End Solution:\n",
    "- A cloud data warehouse which supports high performance analytical purposes.\n",
    "- An Amazon Quicksight dashboard for data visualisation and reporting purposes.\n",
    "\n",
    "### Tools:\n",
    "- Partition Tools:\n",
    "    - Apache Spark - partition U.S. immigration data by city and by yearå\n",
    "- Ingestion tools:\n",
    "    - Python Pandas\n",
    "- Scheduling tools:\n",
    "    - Apache Airflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "### U.S. Immigration Dataset (step 2 - Data exploration)\n",
    "\n",
    "<span style=\"color: red\">smaller <code>.csv</code> dataset will be used in the exploration step, and the original <code>.sas7bdat</code> dataset will be used in the cleansing step</span>\n",
    "\n",
    "The U.S. Immigration dataset comes fromt the U.S. National Tourism and Trade Office. The dataset used in this project is from the year 2016.\n",
    "\n",
    "<strong>Main infromation inlcluded:</strong>\n",
    "\n",
    "* The date an immigrant arrive in the U.S.\n",
    "* The country of origin of an immigrant.\n",
    "* The port at which an immigrant arrived.\n",
    "* The address at which an immigrant lived in the U.S.\n",
    "* The airline which an immigrant used to arrive in the U.S.\n",
    "* The flight number which an immigrant used to arrive in the U.S.\n",
    "* The visa type of an immigrant.\n",
    "\n",
    "\n",
    "<strong>Main data quality issues:</strong>\n",
    "    \n",
    "* Missing values\n",
    "* Typing issues: field such as year and month shall be integer type - casting is required\n",
    "* Invalid values issue: the data dictionary defines valid values for fields such as \"i94cit\", \"i94res\", and \"i94port\"\n",
    "* Duplicated records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_fname = \"immigration_data_sample.csv\"\n",
    "df_immigration = pd.read_csv(immigration_fname)\n",
    "df_immigration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### U.S. Immigration Dataset (step 2 - Data cleansing and upload to S3 Datalake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "match_i94cit = create_map([lit(x) for x in chain(*i94cit_i94res_map.items())])\n",
    "match_i94res = create_map([lit(x) for x in chain(*i94cit_i94res_map.items())])\n",
    "match_i94port = create_map([lit(x) for x in chain(*i94port_map.items())])\n",
    "match_i94mode = create_map([lit(x) for x in chain(*i94mode_map.items())])\n",
    "get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(float(x))).isoformat() if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat is processed.\n",
      "../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat is processed.\n"
     ]
    }
   ],
   "source": [
    "df_immigration_master = None\n",
    "is_first = True\n",
    "\n",
    "# only the following columns are within scope of this project:\n",
    "chosen_columns = [\"cicid\", \"i94yr\", \"i94mon\", \"i94cit\", \"i94res\", \"i94port\", \n",
    "                  \"arrdate\", \"i94mode\", \"i94addr\", \"depdate\", \"i94bir\", \"i94visa\",\n",
    "                  \"matflag\", \"biryear\", \"gender\", \"airline\", \"fltno\", \"visatype\"]\n",
    "for file in glob.glob(\"./data/18-83510-I94-Data-2016/*.sas7bdat\"):\n",
    "    # read in dataset into spark dataframe\n",
    "    df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "    \n",
    "    # filter out unnecessary columns\n",
    "    df_immigration = df_immigration.select(*chosen_columns)\n",
    "    \n",
    "    # cast SAS date to iso date\n",
    "    df_immigration = df_immigration.withColumn(\"arrdate\", get_date(df_immigration.arrdate)).withColumn(\"depdate\", get_date(df_immigration.depdate))\n",
    "    \n",
    "    # cast columns into the correct datatypes\n",
    "    df_immigration = df_immigration.withColumn(\"cicid\", col(\"cicid\").cast(IntegerType())).withColumn(\"i94yr\", col(\"i94yr\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"i94mon\", col(\"i94mon\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"i94visa\", col(\"i94visa\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"biryear\", col(\"biryear\").cast(IntegerType())).withColumn(\"i94bir\", col(\"i94bir\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"i94cit\", col(\"i94cit\").cast(IntegerType())).withColumn(\"i94res\", col(\"i94res\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"i94mode\", col(\"i94mode\").cast(IntegerType()))\n",
    "    \n",
    "    # drop records that contain null value in any column\n",
    "    df_immigration = df_immigration.dropna()\n",
    "    \n",
    "    # drop duplicates\n",
    "    df_immigration = df_immigration.distinct()\n",
    "    \n",
    "    # filter out invalid values\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94addr.isin(*valid_i94addr))\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(*valid_i94port))\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94cit.isin(*valid_i94res_i94cit))\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94res.isin(*valid_i94res_i94cit))\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94mode.isin(*valid_i94mode))\n",
    "    \n",
    "    df_immigration = df_immigration.withColumn(\"i94cit\", col(\"i94cit\").cast(StringType())).withColumn(\"i94res\", col(\"i94res\").cast(StringType())) \\\n",
    "                                    .withColumn(\"i94mode\", col(\"i94mode\").cast(StringType()))\n",
    "    \n",
    "    # match code to values\n",
    "    df_immigration = df_immigration.withColumn(\"i94cit\", match_i94cit[df_immigration[\"i94cit\"]].alias(\"i94cit\")) \\\n",
    "                                .withColumn(\"i94res\", match_i94res[df_immigration[\"i94res\"]].alias(\"i94res\")) \\\n",
    "                                .withColumn(\"i94port\", match_i94port[df_immigration[\"i94port\"]].alias(\"i94port\")) \\\n",
    "                                .withColumn(\"i94mode\", match_i94mode[df_immigration[\"i94mode\"]].alias(\"i94mode\"))\n",
    "    \n",
    "    if is_first:\n",
    "        is_first = False\n",
    "        df_immigration_master = df_immigration\n",
    "    else:\n",
    "        df_immigration_master = df_immigration_master.union(df_immigration)\n",
    "    print(f\"{file} is processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used = 1803.5672590732574 seconds.\n"
     ]
    }
   ],
   "source": [
    "# upload to S3 datalake in parquet format\n",
    "s1 = time.time()\n",
    "df_immigration_master.coalesce(1).write.partitionBy(\"i94yr\", 'i94addr').mode('overwrite').parquet(path=os.path.join(datalake_url, 'us_immigration'))\n",
    "print(\"time used = {} seconds.\".format(time.time() - s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "### U.S. Cities Demographics (Step 2 - Data Exploration)\n",
    "\n",
    "This dataset comes from OpenDataSoft, it contains main statistics of demographics of U.S. cities.\n",
    "\n",
    "Main information:\n",
    "\n",
    "* City name\n",
    "* State\n",
    "* Median Age\n",
    "* Total Population\n",
    "* Number of Foreign-born\n",
    "* Race\n",
    "* State Code\n",
    "\n",
    "Main data quality issues:\n",
    "\n",
    "* Duplicated records\n",
    "* All fields are of the type String, which is incorrect\n",
    "* Some recordes contain null values\n",
    "* field names contain spaces - rename required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_fname = \"us-cities-demographics.csv\"\n",
    "df_us_cities = pd.read_csv(demographic_fname, sep=\";\")\n",
    "df_us_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### U.S. Cities Demographics Dataset (Step 2 - Data cleansing and upload to S3 Datalake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_cities = spark.read.format(\"csv\").option(\"header\", \"true\").option( \"delimiter\",\";\").load(demographic_fname)\n",
    "# drop records with null values\n",
    "df_us_cities = df_us_cities.dropna()\n",
    "# drop duplicate records\n",
    "df_us_cities = df_us_cities.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cast all fields to correct data type\n",
    "df_us_cities = df_us_cities.withColumn(\"Median Age\", col(\"Median Age\").cast(DoubleType())) \\\n",
    "                        .withColumn(\"Male Population\", col(\"Male Population\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"Female Population\", col(\"Female Population\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"Number of Veterans\", col(\"Number of Veterans\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"Foreign-born\", col(\"Foreign-born\").cast(IntegerType())) \\\n",
    "                        .withColumn(\"Average Household Size\", col(\"Average Household Size\").cast(DoubleType())) \\\n",
    "                        .withColumn(\"Count\", col(\"Count\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename all fields to remove spaces\n",
    "df_us_cities = df_us_cities.withColumnRenamed(\"City\", \"city\") \\\n",
    "                        .withColumnRenamed(\"State\", \"state\") \\\n",
    "                        .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "                        .withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    "                        .withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    "                        .withColumnRenamed(\"Total Population\", \"total_population\") \\\n",
    "                        .withColumnRenamed(\"Number of Veterans\", \"num_of_veterans\") \\\n",
    "                        .withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    "                        .withColumnRenamed(\"Average Household Size\", \"average_household_size\") \\\n",
    "                        .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "                        .withColumnRenamed(\"Race\", \"race\") \\\n",
    "                        .withColumnRenamed(\"Count\", \"count\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+-----------------+----------------+---------------+------------+----------------------+----------+--------------------+------+\n",
      "|      city|     state|median_age|male_population|female_population|total_population|num_of_veterans|foreign_born|average_household_size|state_code|                race| count|\n",
      "+----------+----------+----------+---------------+-----------------+----------------+---------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Lynwood|California|      29.4|          35634|            36371|           72005|            776|       28061|                  4.43|        CA|Black or African-...|  5346|\n",
      "| Hollywood|   Florida|      41.4|          75358|            74363|          149721|           6056|       55158|                  2.65|        FL|               White|107916|\n",
      "|   Fremont|California|      38.3|         114383|           117808|          232191|           4629|      109427|                  3.12|        CA|  Hispanic or Latino| 29594|\n",
      "|Atascocita|     Texas|      32.8|          37424|            40816|           78240|           4416|        8657|                  3.03|        TX|Black or African-...| 21491|\n",
      "|   Brandon|   Florida|      36.1|          55679|            58289|          113968|           9417|       16390|                  2.64|        FL|American Indian a...|  1921|\n",
      "+----------+----------+----------+---------------+-----------------+----------------+---------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_cities.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used = 16.081935167312622 seconds.\n"
     ]
    }
   ],
   "source": [
    "# upload to s3 datalake in parquet format\n",
    "s2 = time.time()\n",
    "df_us_cities.coalesce(1).write.partitionBy(\"state_code\").mode('overwrite').parquet(path=os.path.join(datalake_url, 'us_cities_demographics'))\n",
    "print(\"time used = {} seconds.\".format(time.time() - s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Codes (Step 2 - Data exploration)\n",
    "\n",
    "The airport code dataset comes from <a href=\"https://datahub.io/core/airport-codes#data\">datahub.io</a>\n",
    "\n",
    "Main information:\n",
    "\n",
    "* Airport type\n",
    "* Airport name\n",
    "* The country of the airport\n",
    "* The state of the airport\n",
    "* The coordinates of the airports\n",
    "\n",
    "Main data quality issues:\n",
    "\n",
    "* elevation_ft is of the type string - this should be integer type\n",
    "* contains non-U.S. airport code - this project only considers U.S. data\n",
    "* state code can be extracted from the \"iso_region\" field\n",
    "* longitude and latitude need to be extracted from \"coordinates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_code_fname = \"airport-codes_csv.csv\"\n",
    "df_airport_codes = pd.read_csv(airport_code_fname)\n",
    "df_airport_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Codes Dataset (Step 2 - Data cleansing and upload to S3 Datalake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes = spark.read.format(\"csv\").option(\"header\", \"true\").load(airport_code_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define udf for new columns\n",
    "extract_state_code = udf(lambda iso_region: iso_region.split('-')[-1])\n",
    "extract_longitude = udf(lambda coordinates: coordinates.split(',')[0])\n",
    "extract_latitude = udf(lambda coordinates: coordinates.split(',')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract information for new columns\n",
    "df_airport_codes = df_airport_codes.withColumn(\"state_code\", extract_state_code(df_airport_codes.iso_region)) \\\n",
    "                                    .withColumn(\"longitude\", extract_longitude(df_airport_codes.coordinates)) \\\n",
    "                                    .withColumn(\"latitude\", extract_latitude(df_airport_codes.coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter out non-U.S. data\n",
    "df_airport_codes = df_airport_codes.filter(df_airport_codes.iso_country==\"US\")\n",
    "# drop unnecessary columns\n",
    "df_airport_codes = df_airport_codes.drop(\"iso_region\", \"coordinates\")\n",
    "# drop duplicates\n",
    "df_airport_codes = df_airport_codes.distinct()\n",
    "# cast columns to correct data types\n",
    "df_airport_codes = df_airport_codes.withColumn(\"elevation_ft\", col(\"elevation_ft\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"longitude\", col(\"longitude\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"latitude\", col(\"latitude\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used = 18.05463218688965 seconds.\n"
     ]
    }
   ],
   "source": [
    "# upload to S3 datalake\n",
    "s3 = time.time()\n",
    "df_airport_codes.coalesce(1).write.partitionBy(\"state_code\").mode('overwrite').parquet(path=os.path.join(datalake_url, 'airport_codes'))\n",
    "print(\"time used = {} seconds.\".format(time.time() - s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
